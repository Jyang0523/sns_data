
from bs4 import BeautifulSoup     
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
import requests 
import time
import csv
import re

# 크롬 드라이버 주소
path = "C:/Users/kaoka/big_data_subject/SNS_data/chromedriver.exe"
driver = webdriver.Chrome(path)

driver.get("https://www.google.com/?authuser=0")
query_txt = input('크롤링할 키워드는 무엇입니까?: ')

element=driver.find_element(By.NAME, 'q').click()
driver.find_element(By.NAME, 'q').send_keys('filetype:pdf '+query_txt)
time.sleep(1)

element=driver.find_element(By.XPATH, '/html/body/div[1]/div[3]/form/div[1]/div[1]/div[4]/center/input[1]').click()
# element=driver.find_element(By.XPATH, '/html/body/div[1]/div[3]/form/div[1]/div[1]/div[2]/div[2]/div[5]/center/input[1]').click()
time.sleep(1)

pdf_list = []
html = driver.page_source
soup = BeautifulSoup(html, 'html.parser')
content_list = soup.find_all(class_ = 'MjjYud')
for i in range(1,len(content_list)):
    const_str = str(content_list[i])
    pattern  = r'(?i)\bhttps?://\S+\.pdf\b'
    if(re.search(pattern, const_str) == None):
        print('error')
    else:
        href_value = re.search(pattern, const_str).group().strip('"')
        pdf_list.append(href_value)
    
pdf_list

# 저장주소
path = r'저장주소'
for i in range(len(pdf_list)):
    url = str(pdf_list[i])
    try:
        response = requests.get(url)
        
        with open(path+str(i)+'.pdf', "wb") as file:
            file.write(response.content)
    except:
        print("다운로드 불가능한 pdf입니다")
